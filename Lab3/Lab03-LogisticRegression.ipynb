{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03: Logistic Regression.\n",
    "\n",
    "- Student ID: 21127099\n",
    "- Student name: Nguyễn Tấn Lộc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
    "\n",
    "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - that is the average value of a pixel in an image for the different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.17740512 19.40680177 38.03420776 36.15420938 30.99599983 32.95015873\n",
      " 35.23486491 29.21798737 38.39790125 31.35940809]\n"
     ]
    }
   ],
   "source": [
    "labels=np.unique(y)\n",
    "# print(labels)\n",
    "n_label=np.unique(y).shape[0]\n",
    "l_means=np.zeros(shape=n_label,dtype=float) #array stores average intensity for each label\n",
    "\n",
    "#TODO compute average intensity for each label\n",
    "# for label in labels:\n",
    "#     label_pixels = X[y == label]\n",
    "#     label_mean_intensity = np.mean(label_pixels)\n",
    "#     l_means[int(label)] = label_mean_intensity\n",
    "for i in range(n_label):\n",
    "    label_index = labels[i]\n",
    "    label_data = X[y == label_index]\n",
    "    mean_intensity = np.mean(label_data)\n",
    "    l_means[i] = mean_intensity\n",
    "print(l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average intensity using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "#TODO compute average intensity for each data sample\n",
    "# intensity=?\n",
    "intensity = np.mean(X, axis=1)\n",
    "print(intensity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people really do not know what are they doing. I am not an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X_flip=np.flip(X)\n",
    "symmetry= np.mean((X-X_flip),axis=1)\n",
    "print(symmetry.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this feature work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n"
     ]
    }
   ],
   "source": [
    "#TODO create X_new by horizontal stack intensity and symmetry\n",
    "X_new = np.column_stack((intensity, symmetry))\n",
    "print(X_new.shape) #it should be (70000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "First normalize data using Z-score normalization\n",
    "- **TODO: Study about Z-score normalization**\n",
    "    - Z-score normalization, also known as standardization, is a statistical method used to transform data into a standard normal distribution with a mean of 0 and a standard deviation of 1. This normalization technique is particularly useful when the features in a dataset have different scales and units.\n",
    "    - The formula for Z-score normalization for a given value $X$ in a dataset is:\n",
    "    $$X_{normalized}=\\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "    - Where:\n",
    "        - $ X $ is the original value.\n",
    "        - $ \\mu $ is the mean of the dataset.\n",
    "        - $ \\sigma $ is the standard deviation of the dataset.\n",
    "    - The process involves subtracting the mean $(\\mu)$ from each value in the dataset and then dividing by the standard deviation $(\\sigma)$. This centers the data around 0 and scales it based on the variability of the dataset.\n",
    "- **TODO: Why should we normalize data?**\n",
    "    - When working with data, normalizing information is a important step to achieve `balance and efficiency` in processing, analyzing, and applying Machine Learning (ML) algorithms. The Z-score normalization method is one of the popular techniques used to standardize data to the same scale and ensure that the data follows a **normal distribution**.\n",
    "\n",
    "    - When data is not normalized, attributes often have different ranges or units, making comparisons or their utilization in algorithms challenging. By applying Z-score normalization, we can bring attribute values to the same range, creating a balance between them, which facilitates analysis and processing.\n",
    "\n",
    "    - Besides, normalization helps minimize the influence of Noise and Outliers. These values can distort data and affect analysis. Z-score normalization addresses the variance caused by mean values on individual data points, reducing significant changes in these values.\n",
    "\n",
    "    - Furthermore, standardizing data to a normal (Gaussian) distribution with a mean of 0 and a standard deviation of 1 enhances the efficiency of algorithms like linear regression and logistic regression. This aligns them with assumptions about data distribution, thereby improving their performance.\n",
    "\n",
    "    - In summary, normalizing data using Z-score not only creates balance among attributes but also reduces Noise, mitigates the impact of Outliers, and enhances the performance of Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85342722 -1.6193639 ]\n",
      " [ 1.06445354 -0.50547592]\n",
      " [ 0.37569199 -1.35218211]\n",
      " ...\n",
      " [ 0.95427061 -0.19502487]\n",
      " [ 0.79632598 -1.04173105]\n",
      " [ 1.69918763  0.07215692]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: normalize X_new\n",
    "X_normalized = (X_new - np.mean(X_new)) / np.std(X_new)\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 3)\n",
      "(70000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
    "y_new=y.astype(int)\n",
    "y_new=y_new.to_numpy()\n",
    "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
    "y_new=y_new.reshape(-1,1)\n",
    "print (X_new.shape)\n",
    "print (y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46667, 3)\n",
      "(46667, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "def sigmoid_deriv(x):\n",
    "    '''compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input ‘x‘ has already been passed through the sigmoid\n",
    "    activation function'''\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(W, X):\n",
    "    \"\"\"\n",
    "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
    "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    return sigmoid_activation(X.dot(W))\n",
    "def predict(W, X):\n",
    " \n",
    "    '''Take the dot product between our features and weight matrix, \n",
    "       then pass this value through our sigmoid activation'''\n",
    "    #........\n",
    "    preds=sigmoid_activation(X.dot(W))\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds <= 0.5] = 0\n",
    "    preds[preds > 0] = 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function: Average negative log likelihood**\n",
    "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
    "\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(train_X, error):\n",
    "    \"\"\"\n",
    "    This is the gradient descent update of \"average negative loglikelihood\" loss function. \n",
    "    In lab02 our loss function is \"sum squared error\".\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    gradient = np.dot(train_X.T, error)\n",
    "    gradient/=len(train_X)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
    "    for epoch in np.arange(0, num_epochs):\n",
    "        h=compute_h(W,train_X)\n",
    "        error = h - train_y\n",
    "        # If EPSILON isn't used, the code may trigger RuntimeWarning for division by zero in logarithmic operations and for encountering invalid values during multiplication. \n",
    "        loss = np.mean(- train_y * np.log(h + EPSILON) - (1 - train_y) * np.log(1 - h + EPSILON))\n",
    "        losses.append(loss)\n",
    "        gradient=compute_gradient(train_X, error)\n",
    "        W += -learning_rate * gradient\n",
    "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, loss 0.294\n",
      "Epoch 2000, loss 0.287\n",
      "Epoch 3000, loss 0.282\n",
      "Epoch 4000, loss 0.276\n",
      "Epoch 5000, loss 0.271\n",
      "Epoch 6000, loss 0.266\n",
      "Epoch 7000, loss 0.262\n",
      "Epoch 8000, loss 0.258\n",
      "Epoch 9000, loss 0.254\n",
      "Epoch 10000, loss 0.250\n",
      "Epoch 11000, loss 0.247\n",
      "Epoch 12000, loss 0.244\n",
      "Epoch 13000, loss 0.241\n",
      "Epoch 14000, loss 0.238\n",
      "Epoch 15000, loss 0.236\n",
      "Epoch 16000, loss 0.234\n",
      "Epoch 17000, loss 0.231\n",
      "Epoch 18000, loss 0.229\n",
      "Epoch 19000, loss 0.228\n",
      "Epoch 20000, loss 0.226\n",
      "Epoch 21000, loss 0.224\n",
      "Epoch 22000, loss 0.223\n",
      "Epoch 23000, loss 0.221\n",
      "Epoch 24000, loss 0.220\n",
      "Epoch 25000, loss 0.219\n",
      "Epoch 26000, loss 0.218\n",
      "Epoch 27000, loss 0.216\n",
      "Epoch 28000, loss 0.215\n",
      "Epoch 29000, loss 0.215\n",
      "Epoch 30000, loss 0.214\n",
      "Epoch 31000, loss 0.213\n",
      "Epoch 32000, loss 0.212\n",
      "Epoch 33000, loss 0.211\n",
      "Epoch 34000, loss 0.211\n",
      "Epoch 35000, loss 0.210\n",
      "Epoch 36000, loss 0.209\n",
      "Epoch 37000, loss 0.209\n",
      "Epoch 38000, loss 0.208\n",
      "Epoch 39000, loss 0.208\n",
      "Epoch 40000, loss 0.207\n",
      "==================================================\n",
      "Train err of final w:  8.58208155656031\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(train_X.shape[1], 1)\n",
    "losses=[]\n",
    "num_epochs=40000\n",
    "learning_rate=0.01\n",
    "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
    "x_preds=predict(W ,train_X)\n",
    "train_err = np.mean(x_preds != train_y) * 100\n",
    "print ('=' * 50)\n",
    "print ('Train err of final w: ', train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     41445\n",
      "           1       0.77      0.33      0.46      5222\n",
      "\n",
      "    accuracy                           0.91     46667\n",
      "   macro avg       0.85      0.66      0.71     46667\n",
      "weighted avg       0.90      0.91      0.90     46667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, train_X)\n",
    "print(classification_report(train_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     20678\n",
      "           1       0.78      0.33      0.46      2655\n",
      "\n",
      "    accuracy                           0.91     23333\n",
      "   macro avg       0.85      0.66      0.71     23333\n",
      "weighted avg       0.90      0.91      0.90     23333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, test_X)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Comment on the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `Training and Test set` accuracy scores stand high, approximately 0.92, suggesting that the Logistic Regression model applied to this dataset might not suffer from ***Overfitting*** or ***Underfitting***. This signifies the model's efficient performance, indicating its capability to predict accurately on new data.\n",
    "\n",
    "However, the model displays a notable precision and accuracy imbalance coupled with low recall, particularly evident in its handling of the Negative (class 0) and Positive (class 1) instances. The model excels in predicting Negative cases accurately but struggles with a considerable number of False Negatives for Positive cases, leading to a compromised Recall score.\n",
    "\n",
    "Looking at precision and recall more closely:\n",
    "\n",
    "    - For Negative class (0), both precision and recall are high, indicating the model's proficiency in identifying these instances.\n",
    "    - Conversely, for the Positive class (1), precision stands at around 75%, while recall is lower, approximately 37%, suggesting the model's tendency to miss certain Positive instances.\n",
    "The F1-scores provide a balanced view, demonstrating a reasonable weighted average for both classes, considering the trade-off between precision and recall.\n",
    "\n",
    "While the overall accuracy is around 91% on the test set, it might be heavily influenced by the larger class (class 0). The macro and weighted averages present a balanced performance overview, but the lower recall for class 1 contributes to a reduced macro-average.\n",
    "\n",
    "Improving the model's performance could involve:\n",
    "\n",
    "    - Addressing class imbalance using techniques like oversampling the minority class or adjusting class weights.\n",
    "    - Exploring hyperparameter tuning to optimize the model's behavior, experimenting with different logistic regression parameters.\n",
    "    - Considering more advanced techniques such as ensemble methods or alternative classification algorithms, especially if the dataset complexity demands it.\n",
    "In summary, while the model demonstrates good performance for the majority class, enhancing its ability to capture instances of the minority class is crucial. Further exploration, tuning, and potentially exploring alternative methodologies could significantly improve the model's overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
